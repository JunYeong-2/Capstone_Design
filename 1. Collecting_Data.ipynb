{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOegU10nA8qz8TM9GD1tl0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gayeon6423/BusinessAI-Capston/blob/main/1_Collecting_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timeline\n",
        "### 1. Import library\n",
        "### 2. Crawling News Data\n",
        "- [2.1 Generate date list](#2.1-Generate-date-list)\n",
        "- [2.2 Generate news title, news url list](#2.2-Generate-news-title,-news-url-list)\n",
        "- [2.3 Generate news dataframe](#2.3-Generate-news-dataframe)\n",
        "### 3. Collecting Numeric Data"
      ],
      "metadata": {
        "id": "xf-Wz0-CYSPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import library"
      ],
      "metadata": {
        "id": "CpI7rntuYh_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모듈 설치\n",
        "# !pip install konlpy\n",
        "# !pip install -U finance-datareader\n",
        "\n",
        "# 데이터 처리 모듈\n",
        "import pandas as pd\n",
        "import copy\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "# 텍스트 관련 모듈\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "from konlpy.tag import *\n",
        "import nltk\n",
        "# 데이터 수집 모듈\n",
        "import requests\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "zX1tN_xpYjfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Crawling News Data"
      ],
      "metadata": {
        "id": "YgaANcdSYw-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.1 Generate date list"
      ],
      "metadata": {
        "id": "nijQRyUpYzIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_date_list(startdate, enddate):\n",
        "  # 시작일과 종료일을 datetime 형식으로 변환\n",
        "  start = datetime.strptime(startdate, \"%Y%m\")\n",
        "  end = datetime.strptime(enddate, \"%Y%m\")\n",
        "\n",
        "  # 시작일부터 종료일까지의 날짜 리스트를 생성\n",
        "  date_list = []\n",
        "  current = start\n",
        "  while current <= end:\n",
        "    # 현재 날짜를 \"%Y%m%d\" 형식의 문자열로 변환하여 리스트에 추가\n",
        "    date_list.append(current.strftime(\"%Y%m%d\"))\n",
        "\n",
        "    # 현재 날짜를 하루씩 증가\n",
        "    current += relativedelta(days=1)\n",
        "\n",
        "    # 다음 날짜의 월이 현재 날짜와 다른 경우, 다음 달의 첫 날로 변경\n",
        "    if current.month != (current + relativedelta(days=1)).month:\n",
        "      current = current.replace(day=1) + relativedelta(months=1) # relativedelta : 1달 더함\n",
        "\n",
        "  return date_list"
      ],
      "metadata": {
        "id": "3ZanlZEJZD3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.2 Generate news title, news url list"
      ],
      "metadata": {
        "id": "GG1-No71ZHUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate news title, news url LIST function\n",
        "\n",
        "def generate_news_list(date_list):\n",
        "    news_date_li = []\n",
        "    # 웹 페이지 크롤링 할 때 사용할 페이지 클래스 타입\n",
        "    page_class_type = ['type06_headline', 'type06']\n",
        "\n",
        "    # 날짜 리스트인 date_list를 반복하면서 뉴스 수집\n",
        "    for date in tqdm(date_list):\n",
        "\n",
        "    # 날짜별 뉴스 목록을 수집할 URL 생성\n",
        "        url = f'https://news.naver.com/main/list.naver?mode=LS2D&sid2=258&mid=shm&sid1=101&date={date}&page=999'\n",
        "\n",
        "        # HTTP 요청 헤더 설정\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n",
        "        # HTTP GET 요청을 보내서 웹 페이지의 내용 가져옴\n",
        "        raw = requests.get(url, headers=headers)\n",
        "\n",
        "        # BeautifulSoup으르 사용해서 웹 페이지의 HTML 파싱\n",
        "        soup = BeautifulSoup(raw.text, \"html.parser\")\n",
        "\n",
        "        # 해당 날짜에 해당하는 뉴스 목록의 최대 페이지 수\n",
        "        max_page = int(soup.select(\"#main_content > div.paging > strong\")[0].text)\n",
        "\n",
        "        # 각 페이지에서 뉴스 정보 수집\n",
        "        news_page_li = []\n",
        "        for page in tqdm(range(1, max_page + 1)):\n",
        "            for ct in page_class_type:\n",
        "                for news in range(1, 11): # 한 페이지당 10개의 기사\n",
        "                    # 페이지별로 뉴스 목록을 가져오는 URL 생성\n",
        "                    url = f'https://news.naver.com/main/list.naver?mode=LS2D&sid2=259&mid=shm&sid1=101&date={date}&page={page}'\n",
        "                    # HTTP GET 요청을 보내서 해당 페이지의 뉴스 목록을 가져옴\n",
        "                    raw = requests.get(url, headers=headers)\n",
        "                    soup = BeautifulSoup(raw.text, \"html.parser\")\n",
        "\n",
        "                    # HTML에서 뉴스 제목과 URL 추출\n",
        "                    news_all = soup.select(f\"#main_content > div.list_body.newsflash_body > ul.{ct} > li:nth-child({news}) > dl > dt:nth-child(2) > a\")\n",
        "                    news_dic = {}\n",
        "\n",
        "                    # 뉴스 정보가 있는 경우 news_dic 딕셔너리에 key값과 value 매핑\n",
        "                    if news_all:\n",
        "                        news_dic['title'] = news_all[0].text.strip()\n",
        "                        news_dic['url'] = news_all[0]['href']\n",
        "                        news_dic['pubdate'] = date\n",
        "\n",
        "                    # 뉴스 정보가 없는 경우 None값을 value로 설정\n",
        "                    else:\n",
        "                        news_dic['title'] = None\n",
        "                        news_dic['url'] = None\n",
        "                        news_dic['pubdate'] = None\n",
        "\n",
        "                    # 존재하는 뉴스 정보인 경우 뉴스 리스트에 추가\n",
        "                    if news_dic['title'] is not None and news_dic['url'] is not None:\n",
        "                        news_page_li.append(news_dic)\n",
        "\n",
        "        # 해당 날짜의 모든 뉴스 정보를 저장한 리스트를 전체 뉴스 리스트에 추가\n",
        "        news_date_li.extend(news_page_li)\n",
        "\n",
        "    return news_date_li"
      ],
      "metadata": {
        "id": "m2jMdqp4ZJlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.3 Generate news dataframe"
      ],
      "metadata": {
        "id": "kBbK7x6LZLbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate news title, url, content, pundate DataFrame Function\n",
        "\n",
        "def generate_news_content_df(news_date_li):\n",
        "    news_page_li = []\n",
        "\n",
        "    # 뉴스의 URL을 사용하여 뉴스 페이지에 접근하고 내용 스크랩\n",
        "    for news in tqdm(news_date_li):\n",
        "        url = news['url']\n",
        "\n",
        "        # HTTP 요청 헤더 설정\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n",
        "\n",
        "        # HTTP GET 요청을 보내서 뉴스 페이지의 내용을 가져옴\n",
        "        raw = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(raw.text, \"html.parser\")\n",
        "\n",
        "        # HTML에서 뉴스 내용, 날짜 및 시간을 추출\n",
        "        content = soup.select('#dic_area')\n",
        "        pub_time = soup.select('#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div:nth-child(1) > span')\n",
        "\n",
        "        # 뉴스 내용을 딕셔너리에 저장\n",
        "        if content:\n",
        "            news['content'] = content[0].text.strip()\n",
        "            pub_time = pub_time[0].text.strip().replace('오후', 'PM').replace('오전', 'AM')\n",
        "            news['pub_date_time'] = datetime.strptime(pub_time, '%Y.%m.%d. %p %I:%M').strftime('%Y%m%d%H%M')\n",
        "\n",
        "        # 뉴스 내용 없으면 None값 저장\n",
        "        else:\n",
        "            news['content'] = None\n",
        "            news['pub_date_time'] = None\n",
        "\n",
        "        # 스크랩한 뉴스 정보를 리스트에 추가\n",
        "        news_page_li.append(news)\n",
        "\n",
        "    # 뉴스 정보를 데이터프레임으로 변환\n",
        "    news_df = pd.DataFrame(news_page_li)\n",
        "    return news_df"
      ],
      "metadata": {
        "id": "K5d5r6LvZM95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Crawling Month News Data"
      ],
      "metadata": {
        "id": "JVYCiNdBZZ1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2021년1월~2023년6월 데이터 수집\n",
        "for startdate in range(202101,202306):\n",
        "  enddate = str(startdate + 1)\n",
        "  startdate = str(startdate)\n",
        "\n",
        "  # Generate Date List\n",
        "  date_list = generate_date_list(startdate, enddate)\n",
        "\n",
        "  # Generate News title, URL\n",
        "  news_date_li = generate_news_list(date_list)\n",
        "\n",
        "  # Generate News Content\n",
        "  news_df = generate_news_content_df(news_date_li)\n",
        "\n",
        "  # Save News Data\n",
        "  file_path =  f\"/content/drive/MyDrive/산업 AI 캡스톤/DATA/Original_News_Data/경제면_증권섹터_기사({startdate}).csv\"\n",
        "  news_df.to_csv(file_path,encoding='utf-8-sig', index=False)"
      ],
      "metadata": {
        "id": "PBL8lg5Iai9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Collecting Numeric Data"
      ],
      "metadata": {
        "id": "xUkqyaOYckQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- FinanceDataReader : kospi지수 및 미국주가지수 데이터 수집"
      ],
      "metadata": {
        "id": "Y3BbA8KBc4Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kospi = fdr.DataReader('KS11', '2020','2023.06.30').reset_index()\n",
        "dji = fdr.DataReader('DJI', '2020','2023.06.30').reset_index()\n",
        "kosdaq = fdr.DataReader('KQ11', '2020','2023.06.30').reset_index()\n",
        "us500 = fdr.DataReader('US500', '2020','2023.06.30').reset_index()\n",
        "ex_AM = fdr.DataReader('USD/KRW', '2020','2023.06.30').reset_index() #달러당 원화\n",
        "ex_JP = fdr.DataReader('JPY/KRW', '2020','2023.06.30').reset_index() #엔화 원화\n",
        "KOSPI_df= pd.DataFrame(columns=[\"Date\"])\n",
        "KOSDAQ_df = pd.DataFrame(columns=[\"Date\"])\n",
        "USI_df=pd.DataFrame(columns=[\"Date\"])\n",
        "EX_df = pd.DataFrame(columns=[\"Date\"])\n",
        "\n",
        "# 코스피 지수 데이터 데이터프레임\n",
        "KOSPI_df[\"Date\"] = kospi[\"Date\"]\n",
        "KOSPI_df[\"Kospi_open\"] = kospi[\"Open\"]\n",
        "KOSPI_df[\"Kospi_high\"] = kospi[\"High\"]\n",
        "KOSPI_df[\"Kospi_low\"] = kospi[\"Low\"]\n",
        "KOSPI_df[\"Kospi_close\"] = kospi[\"Close\"]\n",
        "KOSPI_df[\"Kospi_vol\"] = kospi[\"Volume\"]\n",
        "\n",
        "# 코스닥 지수 데이터 데이터프레임\n",
        "KOSDAQ_df[\"Date\"] = kosdaq[\"Date\"]\n",
        "KOSDAQ_df[\"kosdaq_open\"] = kosdaq[\"Open\"]\n",
        "KOSDAQ_df[\"kosdaq_high\"] = kosdaq[\"High\"]\n",
        "KOSDAQ_df[\"kosdaq_low\"] = kosdaq[\"Low\"]\n",
        "KOSDAQ_df[\"kosdaq_close\"] = kosdaq[\"Close\"]\n",
        "KOSDAQ_df[\"kosdaq_vol\"] = kosdaq[\"Volume\"]\n",
        "\n",
        "# 미국 주가지수 데이터 데이터프레임\n",
        "USI_df[\"Date\"] = dji[\"Date\"]\n",
        "USI_df[\"dji_open\"] = dji[\"Open\"]\n",
        "USI_df[\"dji_high\"] = dji[\"High\"]\n",
        "USI_df[\"dji_low\"] = dji[\"Low\"]\n",
        "USI_df[\"dji_close\"] = dji[\"Close\"]\n",
        "USI_df[\"dji_vol\"] = dji[\"Volume\"]\n",
        "USI_df[\"us500_open\"] = us500[\"Open\"]\n",
        "USI_df[\"us500_high\"] = us500[\"High\"]\n",
        "USI_df[\"us500_low\"] = us500[\"Low\"]\n",
        "USI_df[\"us500_close\"] = us500[\"Close\"]\n",
        "USI_df[\"us500_vol\"] = us500[\"Volume\"]\n",
        "\n",
        "# 환율 데이터 데이터프레임\n",
        "EX_df[\"Date\"] = ex_AM[\"Date\"]\n",
        "EX_df[\"ex_AM_open\"] = ex_AM[\"Open\"]\n",
        "EX_df[\"ex_AM_high\"] = ex_AM[\"High\"]\n",
        "EX_df[\"ex_AM_low\"] = ex_AM[\"Low\"]\n",
        "EX_df[\"ex_AM_close\"] = ex_AM[\"Close\"]\n",
        "EX_df[\"ex_JP_open\"] = ex_JP[\"Open\"]\n",
        "EX_df[\"ex_JP_high\"] = ex_JP[\"High\"]\n",
        "EX_df[\"ex_JP_low\"] = ex_JP[\"Low\"]\n",
        "EX_df[\"ex_JP_close\"] = ex_JP[\"Close\"]"
      ],
      "metadata": {
        "id": "JIVCDrGGckuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Merge Data"
      ],
      "metadata": {
        "id": "PdikWoLBc_RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kospi_middle_df = pd.merge(KOSPI_df,USI_df,on=\"Date\")\n",
        "kosdaq_middle_df = pd.merge(KOSDAQ_df,USI_df,on=\"Date\")\n",
        "kospi_total_df = pd.merge(kospi_middle_df,EX_df,on=\"Date\")\n",
        "kosdaq_total_df = pd.merge(kosdaq_middle_df,EX_df,on=\"Date\")"
      ],
      "metadata": {
        "id": "JEjqmqQKc3Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한국은행 API : 이자율, 종합소비자물가지수, 종합부동산지수 데이터 수집"
      ],
      "metadata": {
        "id": "yU0muY_cdCtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crollecting ECOS Data Method\n",
        "private_api_key = \"GVFDCZ2JSQ3FWKKD4HD8\"\n",
        "\n",
        "def EcosDownload(Statcode, Freq, Begdate, Enddate, Subcode1, Subcode2, Subcode3):\n",
        "    url = 'http://ecos.bok.or.kr/api/StatisticSearch/%s/xml/kr/1/100000/%s/%s/%s/%s/%s/%s/%s/'%(private_api_key, Statcode, Freq, Begdate, Enddate, Subcode1, Subcode2, Subcode3)\n",
        "    raw = requests.get(url)\n",
        "    xml = BeautifulSoup(raw.text,'xml')\n",
        "\n",
        "    # Pandas 데이터프레임으로 전환합니다.\n",
        "    raw_data = xml.find_all(\"row\")\n",
        "    date_list = []\n",
        "    value_list = []\n",
        "\n",
        "    for item in raw_data:\n",
        "        value = item.find('DATA_VALUE').text.encode('utf-8')\n",
        "        date_str = item.find('TIME').text\n",
        "        value = float(value)\n",
        "        date_list.append(datetime.datetime.strptime(date_str,'%Y%m'))\n",
        "        value_list.append(value)\n",
        "\n",
        "    df = pd.DataFrame(index = date_list)\n",
        "    df['value'] = value_list\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ExyW6OJtdoUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Collecting Data"
      ],
      "metadata": {
        "id": "8bxdOhwDd3h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interest = EcosDownload('722Y001', 'M', '202001', '202307', '0101000','','')\n",
        "consumer = EcosDownload('901Y009', 'M', '202001', '202307', '0', '', '')\n",
        "real_estate = EcosDownload('901Y064', 'M', '202001', '202307', 'P65B','','')\n",
        "\n",
        "# reset index\n",
        "interest = interest.reset_index()\n",
        "consumer = consumer.reset_index()\n",
        "real_estate = real_estate.reset_index()\n",
        "\n",
        "# Preprocess data\n",
        "data_m= pd.DataFrame(columns=[\"Date\"])\n",
        "data_m[\"Date\"] = interest[\"index\"]\n",
        "data_m[\"ko_interest\"] = interest[\"value\"]\n",
        "data_m[\"ko_consumer\"] = consumer[\"value\"]\n",
        "data_m[\"ko_real_estate\"] = real_estate[\"value\"]\n",
        "data_m.set_index('Date', inplace=True)\n",
        "data_m = data_m.resample('D').ffill()\n",
        "data_m.drop(data_m.index[-1], inplace=True)\n",
        "data_m = data_m.reset_index()"
      ],
      "metadata": {
        "id": "rT8gv2CJd0YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Merge Data"
      ],
      "metadata": {
        "id": "6YGdnaG2eJGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_kospi = pd.merge(kospi_total_df,data_m,how='left', left_on='Date', right_on='Date')\n",
        "total_kosdaq = pd.merge(kosdaq_total_df,data_m,how='left', left_on='Date', right_on='Date')"
      ],
      "metadata": {
        "id": "2dzFJWd_eKc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}